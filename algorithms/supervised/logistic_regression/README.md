# Logistic Regression from Scratch

## ğŸ“Œ Problem Definition
Logistic Regression is a supervised learning algorithm used for **binary classification**.  
It models the probability that an input belongs to a particular class using a sigmoid function.

---

## ğŸ“ Mathematical Model

Linear combination:
\[
z = Xw + b
\]

Sigmoid activation:
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

Predicted probability:
\[
\hat{y} = \sigma(Xw + b)
\]

---

## ğŸ“‰ Loss Function (Binary Cross-Entropy)

\[
L = -\frac{1}{n} \sum [y \log(\hat{y}) + (1-y)\log(1-\hat{y})]
\]

This loss penalizes confident but incorrect predictions heavily.

---

## âš™ï¸ Optimization
We use **Gradient Descent** to minimize the loss by computing gradients with respect to:
- Weights \(w\)
- Bias \(b\)

---

## ğŸ§  Key Assumptions
- Linear decision boundary
- Independent features
- Binary target variable

---

## ğŸš§ Limitations
- Cannot model non-linear boundaries without feature engineering
- Sensitive to outliers
